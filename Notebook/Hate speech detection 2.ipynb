{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #15: Twitter hate speech detection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "The goal of our project is to find efficient methods for identifying hate speech on twitter. Our aim is to find a set of features that could be used to identify hate speech content.\n",
    "\n",
    "\n",
    "\n",
    "For our analysis, we have gathered two data sets. The first data set was collected by searching for tweets containing specific hashtags (topics). The second data set was collected from active twitter users that frequently posted hate speech content. Both data sets were obtained using Twitter API and the search-tweets pytho library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys; sys.path.insert(0, '..') # add parent folder to path\n",
    "\n",
    "#3rd. party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom scripts\n",
    "import liwc_empath\n",
    "import util\n",
    "\n",
    "# Dictionary keys\n",
    "CATEGORY_HATE = \"hate_speech_topics\"\n",
    "CATEGORY_NON_HATE = \"non_hate_speech_topics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data sets\n",
    "\n",
    "### Data-set 1: Hate speech hash tags\n",
    "The first data set was collected by searching for tweets containing specific hashtags that were provided to us in the project assignment. The hash tags were: #terrorist, #radicalist, #islamophobia, #extremist, and #bombing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labeled tweets with specific hash tags\n",
    "tweets_hashtag = {}\n",
    "tweets_hashtag[\"bombing\"] = util.read_tweets([\"../Data/tweets_bombing_labeled.json\"])\n",
    "tweets_hashtag[\"extremist\"] = util.read_tweets([\"../Data/tweets_extremist_labeled.json\"])\n",
    "tweets_hashtag[\"islamophobia\"] = util.read_tweets([\"../Data/tweets_islamophobia_labeled.json\"])\n",
    "tweets_hashtag[\"radicalist\"] = util.read_tweets([\"../Data/tweets_radicalist_labeled.json\"])\n",
    "tweets_hashtag[\"terrorist\"] = util.read_tweets([\"../Data/tweets_terrorist_labeled.json\"])\n",
    "\n",
    "print('Hashtag summaries: ')\n",
    "for key in tweets_hashtag.keys():\n",
    "    util.print_hashtag_summary(key, tweets_hashtag[key])\n",
    "\n",
    "# Read the combined labeled data set\n",
    "labeled_tweets = util.read_tweets([\"../Data/tweets_labeled_combined.json\"])\n",
    "print('All labeled tweets:')\n",
    "util.print_hashtag_summary('ALL', labeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Characterization of the labeled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sentiment analysis\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LIWC Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_empath import analyze_tweets_liwc\n",
    "\n",
    "liwc_categories = analyze_tweets_liwc(labeled_tweets)\n",
    "\n",
    "print(\"Top 20 topics in hate tweets:\")\n",
    "for i in range(20):\n",
    "    print(liwc_categories[CATEGORY_HATE][i][0]+ \": \" + str(round(liwc_categories[CATEGORY_HATE][i][1],4)))\n",
    "    \n",
    "print(\"\\nTop 20 topics in non hate tweets:\")\n",
    "for i in range(20):\n",
    "    print(liwc_categories[CATEGORY_NON_HATE][i][0]+ \": \" + str(round(liwc_categories[CATEGORY_NON_HATE][i][1],4)))\n",
    "\n",
    "# Draw top 20 categories    \n",
    "categories_hate = [x for (x,y) in liwc_categories[CATEGORY_HATE][:20]]                                      \n",
    "values_hate = [y for (x,y) in liwc_categories[CATEGORY_HATE][:20]]\n",
    "\n",
    "categories_non_hate = [x for (x,y) in liwc_categories[CATEGORY_NON_HATE][:20]]                                     \n",
    "values_non_hate = [y for (x,y) in liwc_categories[CATEGORY_NON_HATE][:20]]\n",
    "\n",
    "fig, axs = plt.subplots(2,1,figsize=(12,10), sharey=False)\n",
    "\n",
    "axs[0].bar(categories_hate, values_hate)\n",
    "axs[0].set_title(\"Top 20 LIWC Catgories: Hate speech tweets\")\n",
    "axs[0].tick_params(labelrotation=45)\n",
    "\n",
    "axs[1].bar(categories_non_hate, values_non_hate)\n",
    "axs[1].set_title(\"Top 20 LIWC Catgories: Non hate speech tweets\")\n",
    "axs[1].tick_params(labelrotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Emoticon usage\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Named entities\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Named phrases\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Radicalization of active hate speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "from analyse_user import analyse_users\n",
    "\n",
    "first_user = analyse_users('..\\\\Data\\\\tweets_user_ViidarUkonpoika.json')\n",
    "second_user = analyse_users('..\\\\Data\\\\tweets_user_UKInfidel.json')\n",
    "third_user = analyse_users('..\\\\Data\\\\tweets_user_DrDavidDuke.json')\n",
    "\n",
    "\n",
    "#print('file: ' + first_user[\"source_file\"])\n",
    "#print('mean sentiment percentile: ' + str(mean_sentiment_perc))\n",
    "#print('number of negative posts: ' + str(no_neg_posts))\n",
    "#print('volume of negative posts: ' + str(vol_neg_posts))\n",
    "#print('number of very negative posts:' + str(no_very_neg_posts))\n",
    "#print('volume of very negative posts:' + str(vol_very_neg_posts))\n",
    "#print('number of days active: '+ str(time_active.days))\n",
    "#print('radicalization score: '+ str(radicalization_score))\n",
    "#print('very negative post and their sentiments:')\n",
    "#[print(str(item[0]),item[1]) for item in very_neg_tweets_and_sentiments]\n",
    "#df = pd.DataFrame(sentiments, columns = ['sentiment'])\n",
    "#df.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
