{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #15: Twitter hate speech detection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "The goal of our project is to find efficient methods for identifying hate speech on twitter. Our aim is to find a set of features that could be used to identify hate speech content.\n",
    "\n",
    "\n",
    "\n",
    "For our analysis, we have gathered two data sets. The first data set was collected by searching for tweets containing specific hashtags (topics). The second data set was collected from active twitter users that frequently posted hate speech content. Both data sets were obtained using Twitter API and the search-tweets pytho library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys; sys.path.insert(0, '..') # add parent folder to path\n",
    "\n",
    "# Custom scripts\n",
    "import liwc_empath\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data sets\n",
    "\n",
    "### Data-set 1: Hate speech hash tags\n",
    "The first data set was collected by searching for tweets containing specific hashtags that were provided to us in the project assignment. The hash tags were: #terrorist, #radicalist, #islamophobia, #extremist, and #bombing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtag summaries: \n",
      "#bombing: Number of hate speech = 2, Number of non hate speech = 195, Total number of tweets = 197\n",
      "Oldest tweet date: 2019-11-23\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n",
      "#extremist: Number of hate speech = 6, Number of non hate speech = 368, Total number of tweets = 374\n",
      "Oldest tweet date: 2019-11-13\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n",
      "#islamophobia: Number of hate speech = 12, Number of non hate speech = 158, Total number of tweets = 170\n",
      "Oldest tweet date: 2019-12-12\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n",
      "#radicalist: Number of hate speech = 0, Number of non hate speech = 13, Total number of tweets = 13\n",
      "Oldest tweet date: 2019-11-14\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n",
      "#terrorist: Number of hate speech = 117, Number of non hate speech = 334, Total number of tweets = 451\n",
      "Oldest tweet date: 2019-12-11\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n",
      "All labeled tweets:\n",
      "#ALL: Number of hate speech = 137, Number of non hate speech = 1068, Total number of tweets = 1205\n",
      "Oldest tweet date: 2019-11-13\n",
      "Newest tweet date: 2019-12-13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read labeled tweets with specific hash tags\n",
    "tweets_hashtag = {}\n",
    "tweets_hashtag[\"bombing\"] = util.read_tweets([\"../Data/tweets_bombing_labeled.json\"])\n",
    "tweets_hashtag[\"extremist\"] = util.read_tweets([\"../Data/tweets_extremist_labeled.json\"])\n",
    "tweets_hashtag[\"islamophobia\"] = util.read_tweets([\"../Data/tweets_islamophobia_labeled.json\"])\n",
    "tweets_hashtag[\"radicalist\"] = util.read_tweets([\"../Data/tweets_radicalist_labeled.json\"])\n",
    "tweets_hashtag[\"terrorist\"] = util.read_tweets([\"../Data/tweets_terrorist_labeled.json\"])\n",
    "\n",
    "print('Hashtag summaries: ')\n",
    "for key in tweets_hashtag.keys():\n",
    "    util.print_hashtag_summary(key, tweets_hashtag[key])\n",
    "\n",
    "# Read the combined labeled data set\n",
    "labeled_tweets = util.read_tweets([\"../Data/tweets_labeled_combined.json\"])\n",
    "print('All labeled tweets:')\n",
    "util.print_hashtag_summary('ALL', labeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Characterization of the labeled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sentiment analysis\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LIWC Features\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Emoticon usage\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Named entities\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Named phrases\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Radicalization of active hate speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6d9cab47a7bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manalyse_user\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalyse_users\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfirst_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyse_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\\\Data\\\\tweets_user_ViidarUkonpoika.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msecond_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyse_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\\\Data\\\\tweets_user_UKInfidel.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mthird_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyse_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\\\Data\\\\tweets_user_DrDavidDuke.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\NLP2019\\analyse_user.py\u001b[0m in \u001b[0;36manalyse_users\u001b[1;34m(users_source_files)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '.'"
     ]
    }
   ],
   "source": [
    "# CODE HERE\n",
    "from analyse_user import analyse_users\n",
    "\n",
    "first_user = analyse_users('..\\\\Data\\\\tweets_user_ViidarUkonpoika.json')\n",
    "second_user = analyse_users('..\\\\Data\\\\tweets_user_UKInfidel.json')\n",
    "third_user = analyse_users('..\\\\Data\\\\tweets_user_DrDavidDuke.json')\n",
    "\n",
    "\n",
    "print('file: ' + first_user[\"source_file\"])\n",
    "# print('mean sentiment percentile: ' + str(mean_sentiment_perc))\n",
    "# print('number of negative posts: ' + str(no_neg_posts))\n",
    "# print('volume of negative posts: ' + str(vol_neg_posts))\n",
    "# print('number of very negative posts:' + str(no_very_neg_posts))\n",
    "# print('volume of very negative posts:' + str(vol_very_neg_posts))\n",
    "# print('number of days active: '+ str(time_active.days))\n",
    "# print('radicalization score: '+ str(radicalization_score))\n",
    "# print('very negative post and their sentiments:')\n",
    "# [print(str(item[0]),item[1]) for item in very_neg_tweets_and_sentiments]\n",
    "\n",
    "# df = pd.DataFrame(sentiments, columns = ['sentiment'])\n",
    "# df.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
