{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #15: Twitter hate speech detection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "The goal of our project is to find efficient methods for identifying hate speech on twitter. Our aim is to find a set of features that could be used to identify hate speech content.\n",
    "\n",
    "For our analysis, we have gathered two data sets. The first data set was collected by searching for tweets containing specific hashtags (topics). The second data set was collected from active twitter users that frequently posted hate speech content. Both data sets were obtained using Twitter API and the search-tweets pytho library.\n",
    "\n",
    "Full source code and data sets are available at: https://github.com/tokuosma/NLP2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys; sys.path.insert(0, '..') # add parent folder to path\n",
    "\n",
    "#3rd. party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emojis \n",
    "\n",
    "# Custom scripts\n",
    "import liwc_empath\n",
    "import util\n",
    "\n",
    "# Dictionary keys\n",
    "CATEGORY_HATE = \"hate_speech\"\n",
    "CATEGORY_NON_HATE = \"non_hate_speech\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data sets\n",
    "\n",
    "### Data-set 1: Hate speech hash tags\n",
    "The first data set was collected by searching for tweets containing specific hashtags that were provided to us in the project assignment. The hash tags were: #terrorist, #radicalist, #islamophobia, #extremist, and #bombing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labeled tweets with specific hash tags\n",
    "tweets_hashtag = {}\n",
    "tweets_hashtag[\"bombing\"] = util.read_tweets([\"../Data/tweets_bombing_labeled.json\"])\n",
    "tweets_hashtag[\"extremist\"] = util.read_tweets([\"../Data/tweets_extremist_labeled.json\"])\n",
    "tweets_hashtag[\"islamophobia\"] = util.read_tweets([\"../Data/tweets_islamophobia_labeled.json\"])\n",
    "tweets_hashtag[\"radicalist\"] = util.read_tweets([\"../Data/tweets_radicalist_labeled.json\"])\n",
    "tweets_hashtag[\"terrorist\"] = util.read_tweets([\"../Data/tweets_terrorist_labeled.json\"])\n",
    "\n",
    "print('Hashtag summaries: ')\n",
    "for key in tweets_hashtag.keys():\n",
    "    util.print_hashtag_summary(key, tweets_hashtag[key])\n",
    "\n",
    "# Read the combined labeled data set\n",
    "labeled_tweets = util.read_tweets([\"../Data/tweets_labeled_combined.json\"])\n",
    "print('All labeled tweets:')\n",
    "util.print_hashtag_summary('ALL', labeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Characterization of the labeled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sentiment analysis\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LIWC Features\n",
    "\n",
    "To analyze the topics and categories common to hate and non hate speech we use the Empath library. Empath is an open source alternative to proprietary LIWC software. The library offers tools that can extract themes and topics from a given text. The library come with a default set of categories but new categories can be added by the users. We will use the default categories.\n",
    "\n",
    "The following code uses the labeled tweet data set to extract the most common topics from the labeled tweet data sets. Each topic is then given a normalized score which is simply the number of mentions a topic has divided by the total number of tweets in the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_empath import analyze_tweets_liwc\n",
    "\n",
    "liwc_categories = analyze_tweets_liwc(labeled_tweets)\n",
    "\n",
    "print(\"Top 20 topics in hate tweets:\")\n",
    "for i in range(20):\n",
    "    print(liwc_categories[CATEGORY_HATE][i][0]+ \": \" + str(round(liwc_categories[CATEGORY_HATE][i][1],4)))\n",
    "    \n",
    "print(\"\\nTop 20 topics in non hate tweets:\")\n",
    "for i in range(20):\n",
    "    print(liwc_categories[CATEGORY_NON_HATE][i][0]+ \": \" + str(round(liwc_categories[CATEGORY_NON_HATE][i][1],4)))\n",
    "\n",
    "# Draw top 20 categories    \n",
    "categories_hate = [x for (x,y) in liwc_categories[CATEGORY_HATE][:20]]                                      \n",
    "values_hate = [y for (x,y) in liwc_categories[CATEGORY_HATE][:20]]\n",
    "\n",
    "categories_non_hate = [x for (x,y) in liwc_categories[CATEGORY_NON_HATE][:20]]                                     \n",
    "values_non_hate = [y for (x,y) in liwc_categories[CATEGORY_NON_HATE][:20]]\n",
    "\n",
    "fig, axs = plt.subplots(2,1,figsize=(12,10), sharey=False)\n",
    "\n",
    "axs[0].bar(categories_hate, values_hate)\n",
    "axs[0].set_title(\"Top 20 LIWC Catgories: Hate speech tweets\")\n",
    "axs[0].tick_params(labelrotation=90)\n",
    "\n",
    "axs[1].bar(categories_non_hate, values_non_hate)\n",
    "axs[1].set_title(\"Top 20 LIWC Catgories: Non hate speech tweets\")\n",
    "axs[1].tick_params(labelrotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the hashtags that were used to collect the tweets, it is not suprising to find negative and violent topics are present in both data sets. The topic hate is more pronounced in the hate speech data set. Suprisingly, themes like children, family and love are some of the most common topics in the hate data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Emoticon usage\n",
    "We will investigate the usage of emoticons in hate and non hate tweets by examining the types of emoticons used and their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from count_emoticons import count_tweet_emoticons\n",
    "\n",
    "# Retuns two lists containing the types of emoticons and their frequency in both hate and non hate categories\n",
    "tweet_emoticons = count_tweet_emoticons(labeled_tweets)\n",
    "\n",
    "print(\"Emoticon usage in hate and non hate tweets:\\n\")\n",
    "\n",
    "print(\"Total number of emoticons in hate speech tweets: \" + str(sum([v for (k,v) in tweet_emoticons[CATEGORY_HATE]])))\n",
    "print(\"Total number of emoticons in non hate speech tweets: \" + str(sum([v for (k,v) in tweet_emoticons[CATEGORY_NON_HATE]])))\n",
    "\n",
    "print(\"\\nEmoticons in hate speech:\")\n",
    "for emoticon in tweet_emoticons[CATEGORY_HATE]:\n",
    "    print(emojis.encode(emoticon[0]) + \": \" + str(emoticon[1]) + ())\n",
    "\n",
    "print(\"\\nEmoticons in non hate speech:\")\n",
    "for emoticon in tweet_emoticons[CATEGORY_NON_HATE]:\n",
    "    print(emojis.encode(emoticon[0]) + \": \" + str(emoticon[1]))\n",
    "    \n",
    "emoticons_hate = [x for (x,y) in tweet_emoticons[CATEGORY_HATE]]\n",
    "emoticons_counts_hate = [y for (x,y) in tweet_emoticons[CATEGORY_HATE]]\n",
    "\n",
    "emoticons_non_hate = [x for (x,y) in tweet_emoticons[CATEGORY_NON_HATE][:20]]\n",
    "emoticons_counts_non_hate = [y for (x,y) in tweet_emoticons[CATEGORY_NON_HATE][:20]]\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(2,1,figsize=(12,10), sharey=False)\n",
    "\n",
    "axs[0].bar(emoticons_hate, emoticons_counts_hate)\n",
    "axs[0].set_title(\"Emoticon usage in hate speech tweets\")\n",
    "axs[0].tick_params(labelrotation=90)\n",
    "\n",
    "axs[1].bar(emoticons_non_hate, emoticons_counts_non_hate)\n",
    "axs[1].set_title(\"Emoticon usage in non hate speech tweets (Top 20)\")\n",
    "axs[1].tick_params(labelrotation=90)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hate speech tweets only contained a total of 43 emoticons which makes interpreting the results difficult. The most common emoticons used in the hate speech, :joy: and :pout:,  don't really stand out and are frequently used in non hate context, although both seem to be more common in our hate speech data set. \n",
    "\n",
    "The two most common emoticons in the non hate speech data set were :exit: and :bell:. It turns out both emoticons were frequently used by opponents of Jeremy Corbyn, who were active during the British general election that occured in december, at the same time we were collection our data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Named entities\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Named phrases\n",
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Radicalization of active hate speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "from analyse_user import analyse_users\n",
    "\n",
    "first_user = analyse_users('..\\\\Data\\\\tweets_user_ViidarUkonpoika.json')\n",
    "second_user = analyse_users('..\\\\Data\\\\tweets_user_UKInfidel.json')\n",
    "third_user = analyse_users('..\\\\Data\\\\tweets_user_DrDavidDuke.json')\n",
    "\n",
    "\n",
    "#print('file: ' + first_user[\"source_file\"])\n",
    "#print('mean sentiment percentile: ' + str(mean_sentiment_perc))\n",
    "#print('number of negative posts: ' + str(no_neg_posts))\n",
    "#print('volume of negative posts: ' + str(vol_neg_posts))\n",
    "#print('number of very negative posts:' + str(no_very_neg_posts))\n",
    "#print('volume of very negative posts:' + str(vol_very_neg_posts))\n",
    "#print('number of days active: '+ str(time_active.days))\n",
    "#print('radicalization score: '+ str(radicalization_score))\n",
    "#print('very negative post and their sentiments:')\n",
    "#[print(str(item[0]),item[1]) for item in very_neg_tweets_and_sentiments]\n",
    "#df = pd.DataFrame(sentiments, columns = ['sentiment'])\n",
    "#df.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
